{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install all the Libraries"
      ],
      "metadata": {
        "id": "joM_WnAKILOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "gFWu5iuOMM5f"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet python-dotenv gradio\n",
        "!pip install --quiet langchain-unstructured unstructured-client \"unstructured[all-docs]\"\n",
        "!pip install --upgrade --quiet unstructured[local-inference]\n",
        "!pip install --upgrade --quiet nltk\n",
        "!pip install --upgrade --quiet langchain\n",
        "!pip install --quiet langchain-community langchain-core\n",
        "%pip install --upgrade --quiet  langchain sentence_transformers\n",
        "!pip install --quiet langchain-huggingface\n",
        "!pip install --quiet langchain-chroma\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt --local"
      ],
      "metadata": {
        "id": "65rTtZ1HLpUH"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "yCvsuJigqNiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94549b2c-7aae-459d-a77d-9a0276253a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dM2QyDu-qZxz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata/'\n",
        "os.environ['NLTK_DATA'] = '/usr/local/share/nltk_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybrHj-r8h5z-",
        "outputId": "60993fb7-9d8e-4684-86da-22d3b321026b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (14.3 MB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt', download_dir='/usr/local/share/nltk_data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 2. Import all the Libraries"
      ],
      "metadata": {
        "id": "u8kmwjwRGsVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHfJkzm56f83",
        "outputId": "a9b64703-b84d-47c4-f81d-7d09fae770c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "# imports\n",
        "import os\n",
        "import glob\n",
        "# from dotenv import load_dotenv,find_dotenv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import glob\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('Meta_llama_3_1_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = userdata.get(\"Meta_llama_3_1_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NTTP89Z8MFrv"
      },
      "outputs": [],
      "source": [
        "# imports for langchain, plotly and Chroma\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader,PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "# from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_huggingface import ChatHuggingFace,HuggingFaceEmbeddings,HuggingFaceEndpoint\n",
        "from langchain_core.prompts import (ChatPromptTemplate,\n",
        "                                    HumanMessagePromptTemplate,\n",
        "                                    AIMessagePromptTemplate,\n",
        "                                    FewShotChatMessagePromptTemplate)\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_chroma import Chroma\n",
        "import numpy as np\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmqZY01oil9O",
        "outputId": "7d0bb768-1738-45a5-8604-33f9591113a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "55uvcdZ-Nm8h"
      },
      "outputs": [],
      "source": [
        "# price is a factor for our company, so we're going to use a low cost model\n",
        "db_name = \"vector_db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHNbb1mYTX6N",
        "outputId": "7d2a7b12-bf3d-4440-fd4e-2ae1c9d807fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/Manuals and FAQs',\n",
              " '/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/employees',\n",
              " '/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/Company',\n",
              " '/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/Product catalog.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "folders = glob.glob(\"/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/*\")\n",
        "folders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Indexing:"
      ],
      "metadata": {
        "id": "O7jGPH2tIp7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Document Loading, Splitting and Chunking"
      ],
      "metadata": {
        "id": "SwKy6tWXI21L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from langchain_unstructured import UnstructuredLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from unstructured.partition.auto import partition\n",
        "import langdetect\n",
        "\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "def clean_and_filter_english(text):\n",
        "    # Basic cleaning (you might want to add more sophisticated cleaning steps)\n",
        "    cleaned_text = text.strip()\n",
        "\n",
        "    # Detect language and filter for English\n",
        "    try:\n",
        "        if detect(cleaned_text) == 'en':\n",
        "            return cleaned_text\n",
        "    except LangDetectException:\n",
        "        # Handle detection failure\n",
        "        return None\n",
        "\n",
        "    # Return None if the text is not in English\n",
        "    return None\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    elements = partition(filename=file_path)\n",
        "    # Filter for text elements, excluding headers, footers, etc.\n",
        "    text_elements = [el.text for el in elements if el.category == \"NarrativeText\"]\n",
        "    return clean_and_filter_english(\"\\n\\n\".join(text_elements))\n",
        "\n",
        "folders = glob.glob(\"/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/*\")\n",
        "\n",
        "documents = []\n",
        "for item in folders:\n",
        "    doc_type = os.path.basename(item)\n",
        "    if os.path.isdir(item):\n",
        "        for file_path in glob.glob(os.path.join(item, \"**/*.pdf\"), recursive=True):\n",
        "            try:\n",
        "                content = process_pdf(file_path)\n",
        "                if content:\n",
        "                    documents.append({\n",
        "                        \"content\": content,\n",
        "                        \"metadata\": {\"source\": file_path, \"doc_type\": doc_type}\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {str(e)}\")\n",
        "    elif item.lower().endswith('.pdf'):\n",
        "        try:\n",
        "            content = process_pdf(item)\n",
        "            if content:\n",
        "                documents.append({\n",
        "                    \"content\": content,\n",
        "                    \"metadata\": {\"source\": item, \"doc_type\": doc_type}\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {item}: {str(e)}\")\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator=\".\", chunk_size=1000, chunk_overlap=200)\n",
        "chunks = []\n",
        "for doc in documents:\n",
        "    doc_chunks = text_splitter.split_text(doc[\"content\"])\n",
        "    chunks.extend([{\"content\": chunk, \"metadata\": doc[\"metadata\"]} for chunk in doc_chunks])\n",
        "\n",
        "print(f\"Total number of chunks: {len(chunks)}\")\n",
        "print(f\"Document types found: {set(doc['metadata']['doc_type'] for doc in documents)}\")\n",
        "\n",
        "# Print the first chunk as an example\n",
        "if chunks:\n",
        "    print(\"\\nExample chunk:\")\n",
        "    print(f\"Content: {chunks[0]['content'][:200]}...\")\n",
        "    print(f\"Metadata: {chunks[0]['metadata']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0wNNASIacQS",
        "outputId": "66879fe0-4712-4f27-8dc3-532f4536f9f0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:The PDF <_io.BufferedReader name='/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/Manuals and FAQs/Philips Hue Light.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1294, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1340, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1752, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1024, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1725, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2287, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1393, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1297, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2179, which is longer than the specified 1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chunks: 897\n",
            "Document types found: {'Product catalog.pdf', 'Manuals and FAQs'}\n",
            "\n",
            "Example chunk:\n",
            "Content: To download this user manual in a different language, visit gopro.com/support.\n",
            "\n",
            "Wenn Sie dieses Benutzerhandbuch in einer anderen Sprache herunterladen möchten, besuchen Sie gopro.com/support.\n",
            "\n",
            "Para b...\n",
            "Metadata: {'source': '/content/drive/MyDrive/Customer Support Chatbot/Voltathena Knowledge Base/Manuals and FAQs/GoPro Hero.pdf', 'doc_type': 'Manuals and FAQs'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Document Embedding And Storing"
      ],
      "metadata": {
        "id": "2CDhXCN1JPmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import os\n",
        "\n",
        "# Assuming 'chunks' is your list of dictionaries and 'db_name' is defined\n",
        "\n",
        "# Convert chunks to Document objects\n",
        "documents = [Document(page_content=chunk['content'], metadata=chunk['metadata']) for chunk in chunks]\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "# Delete if already exists\n",
        "if os.path.exists(db_name):\n",
        "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
        "\n",
        "# Create vectorstore\n",
        "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUZkdwdVhNvN",
        "outputId": "16d99918-0974-4449-fc2f-e05c9fdec29f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-b50bdc2e5a92>:12: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 622, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2014, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1565, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 813, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/chat_interface.py\", line 638, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"<ipython-input-88-28d1af55d45b>\", line 15, in chat\n",
            "    result = conversation_chain.invoke({\"question\": question})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 170, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 160, in invoke\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\", line 159, in _call\n",
            "    docs = self._get_docs(new_question, inputs, run_manager=_run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/conversational_retrieval/base.py\", line 396, in _get_docs\n",
            "    docs = self.retriever.invoke(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\", line 254, in invoke\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/retrievers.py\", line 247, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\", line 1089, in _get_relevant_documents\n",
            "    docs = self.vectorstore.max_marginal_relevance_search(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\", line 650, in max_marginal_relevance_search\n",
            "    docs = self.max_marginal_relevance_search_by_vector(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\", line 597, in max_marginal_relevance_search_by_vector\n",
            "    results = self.__query_collection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py\", line 53, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\", line 157, in __query_collection\n",
            "    return self._collection.query(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\", line 197, in query\n",
            "    query_results = self._client._query(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\", line 146, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 336, in wrapped_f\n",
            "    return copy(f, *args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 475, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 376, in iter\n",
            "    result = action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
            "    self._add_action_func(lambda rs: rs.outcome.result())\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 478, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/segment.py\", line 675, in _query\n",
            "    coll = self._get_collection(collection_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\", line 146, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/segment.py\", line 761, in _get_collection\n",
            "    raise InvalidCollectionException(\n",
            "chromadb.errors.InvalidCollectionException: Collection a6bddac8-a71a-49e0-9e44-a1db2f9e5db4 does not exist.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorstore created with 897 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKAk5KzYEAa6",
        "outputId": "7749ca00-0192-4cf7-8128-f1a886cddbd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 897 vectors with 768 dimensions in the vector store\n"
          ]
        }
      ],
      "source": [
        "# Let's investigate the vectors\n",
        "collection = vectorstore._collection\n",
        "count = collection.count()\n",
        "\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U0sftINEDn5"
      },
      "source": [
        "## 4. Designing Few-Shot Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "QpDGYNHwfoaX"
      },
      "outputs": [],
      "source": [
        "TEMPLATE_H = '''I'm having an issue with the {product_purchased}.The problem is {problem}.Please assist.'''\n",
        "\n",
        "TEMPLATE_AI = '''Hi, {response}'''\n",
        "\n",
        "message_template_h = HumanMessagePromptTemplate.from_template(template = TEMPLATE_H)\n",
        "message_template_ai = AIMessagePromptTemplate.from_template(template = TEMPLATE_AI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "xVGTEUQyiDts"
      },
      "outputs": [],
      "source": [
        "example_template = ChatPromptTemplate.from_messages([\n",
        "      (\"system\",\"\"\"You are Eric, an AI Assistant for Voltathena, an e-commerce firm selling tech products.\n",
        "Respond in English only. Provide clear, concise answers in an empathetic, human-like tone.\n",
        "If you don't know the answer or if the query is unrelated to products sold at Voltathena, respond with:\n",
        "\"I can only assist you with products sold in Voltathena. Please ask about our products or services.\"\n",
        "Make sure to avoid providing any general knowledge or unrelated information.\"\"\"),\n",
        "                                                      message_template_h,\n",
        "                                                    message_template_ai])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "D_3dPVsJiInr"
      },
      "outputs": [],
      "source": [
        "examples = [{'product_purchased':'Dell XPS laptop', 'problem':'is not turning ON.',\n",
        "             'response':'''Hello there, Let's Troubleshoot your laptop together. Your Dell XPS might not power on due to a drained battery or loose adapter connection.\n",
        "             Try these steps:\n",
        "            - Ensure the power adapter is connected securely.\n",
        "            - Hold the power button for 15-20 seconds to reset the battery.\n",
        "            - Check if the charging LED lights up.\n",
        "            - If these don’t work, please contact Dell Support directly, as they handle in-depth troubleshooting.'''},\n",
        "\n",
        "            {'product_purchased':'Dynamo Vacuum Cleaner','problem':'is not suctioning',\n",
        "             'response':'''Hi there,Let's resolve it.This issue is often caused by blockages or a full dustbin. Try these steps:\n",
        "             Empty the dustbin and clean the filter.Check for clogs in the hose or brush head.Restart the device. If the Problem persists, I can connect you with our Expert.'''},\n",
        "\n",
        "            {'product_purchased':'Nintendo Switch','problem':'delivered wrong Item.Need Refund',\n",
        "             'response':\n",
        "             '''Hello there, Sorry for the Inconvenience Caused. The refund will be credited to your account within 3 to 5 days.'''}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "KoPAqqGmiLBa"
      },
      "outputs": [],
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(examples = examples,\n",
        "                                                   example_prompt = example_template,\n",
        "                                                   input_variables = ['product_purchased','problem'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "GvUIhTHHsnuK"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages([few_shot_prompt,\n",
        "                                                  message_template_h])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Designing the Retrieval Chain"
      ],
      "metadata": {
        "id": "oJLWg0QqK7Zt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "QJs6N2w0ELTb"
      },
      "outputs": [],
      "source": [
        "# Instantiate Zephyr Model through HuggingFace Endpoints\n",
        "\n",
        "model= HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "llm = ChatHuggingFace(llm=model,prompt=chat_template)\n",
        "\n",
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
        "retriever = vectorstore.as_retriever(search_type = \"mmr\",\n",
        "                                     search_kwargs = {\"k\":3,\n",
        "                                                      \"lambda_mult\":0.3})\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever,\n",
        "                                                           memory=memory\n",
        "                                                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generating Response"
      ],
      "metadata": {
        "id": "oSryZD12LJiU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVseG92XEMJk",
        "outputId": "f8c136f6-269c-4bc6-c40a-b216b9c0e2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context provided, the user is experiencing issues with their LG TV. The TV is not turning on and the original charger that came with it is not charging the device properly.\n",
            "\n",
            "To troubleshoot this issue, you can first check if the TV is properly connected to a power source. Make sure that the power cord is securely plugged into both the TV and the wall outlet.\n",
            "\n",
            "If the power cord appears to be working correctly but the TV still won\n"
          ]
        }
      ],
      "source": [
        "# Let's try a simple question\n",
        "\n",
        "query = \"I'm facing a problem with my LG TV.It is not turning on. It was working fine until yesterday, but now it doesn't respond.I really I'm using the original charger that came with it, but it's not charging properly.\"\n",
        "result = conversation_chain.invoke({\"question\": query})\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(query):\n",
        "    greetings = [\"hi\", \"hello\", \"hey\"]\n",
        "    parting_words = [\"Thank You\", \"Thanks\", \"Good Bye\", \"Ok\"]\n",
        "    normalized_query = query.lower().strip()\n",
        "\n",
        "    if any(greet in normalized_query for greet in greetings):\n",
        "        return \"Hello! I'm Eric, Voltathena's AI assistant. How can I help you with your tech product today?\"\n",
        "    elif any(part in normalized_query for part in parting_words):\n",
        "        return \"Thank you for using Voltathena. Have a great day!\"\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def chat(question, history):\n",
        "    greeting_response = preprocess_query(question)\n",
        "    if greeting_response:\n",
        "        return greeting_response\n",
        "\n",
        "    result = conversation_chain.invoke({\"question\": question})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "xFllBBHy9_g3"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Deploy Chat Interface Using Gradio"
      ],
      "metadata": {
        "id": "a9UvJ85tLZ2l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "F0mkOHRT2-Sl",
        "outputId": "d72b5bf0-555d-460a-ed1c-9467a0eb5dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8507a0515d5b29162d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8507a0515d5b29162d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# And in Gradio:\n",
        "import gradio as gr\n",
        "view = gr.ChatInterface(chat,theme='HaleyCH/HaleyCH_Theme').launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxotaBl63CxR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}